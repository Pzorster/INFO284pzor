{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiKnE8nB5vGM"
      },
      "source": [
        "### INFO284 Machine Learning Exam, spring 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Bv9uf39YK_"
      },
      "source": [
        "#### Importing and versioncontrol for relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi0O1bxI9KEG",
        "outputId": "34b6ddad-ba8d-4810-efd0-c55602412e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.12.2 (tags/v3.12.2:6abddd9, Feb  6 2024, 21:26:36) [MSC v.1937 64 bit (AMD64)]\n",
            "pandas version: 2.2.1\n",
            "matplotlib version: 3.8.3\n",
            "NumPy version: 1.26.4\n",
            "SciPy version: 1.12.0\n",
            "IPython version: 8.21.0\n",
            "scikit-learn version: 1.4.1.post1\n",
            "seaborn version: 0.13.2\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(\"Python version: {}\".format(sys.version))\n",
        "import pandas as pd\n",
        "print(\"pandas version: {}\".format(pd.__version__))\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
        "import numpy as np\n",
        "print(\"NumPy version: {}\".format(np.__version__))\n",
        "import scipy as sp\n",
        "print(\"SciPy version: {}\".format(sp.__version__))\n",
        "import IPython\n",
        "print(\"IPython version: {}\".format(IPython.__version__))\n",
        "import sklearn\n",
        "print(\"scikit-learn version: {}\".format(sklearn.__version__))\n",
        "import seaborn as sns\n",
        "print(\"seaborn version: {}\".format(sns.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYRuts8B-ITy"
      },
      "source": [
        "#### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "dHNDvOovwmBw",
        "outputId": "993d640c-7f2d-442f-ee03-7bbb9e05380d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before pre-processing the dataset has 45 columns and 305434 rows\n"
          ]
        }
      ],
      "source": [
        "filePath = 'elektronisk-rapportering-ers-2018-fangstmelding-dca-simple.csv'\n",
        "# Keep in mind that the file is encoded in UTF-8 so it will only work if you have the correct version of pandas.\n",
        "df = pd.read_csv(filePath, encoding=\"UTF-8\" , delimiter=\";\")\n",
        "print(f\"Before pre-processing the dataset has {df.shape[1]} columns and {df.shape[0]} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw5rbR1t-fTc"
      },
      "source": [
        "#### Step 1: chosing target and pre-prossesing\n",
        "After taking some time to understand the data we have chosen our target features to be the catches of Hyse, Torsk and Sei. Next we must pre-prosess the data into suitable forms for modeling, this might take different forms as we try different ways to model it. The key skill here will therefor be the ability to manipulate the pandas dataframe.\n",
        "\n",
        "Ways we might process the data:\n",
        "1. Remove uncessary columns.\n",
        "2. Collapse columns that say the same thing into one column.\n",
        "3. Remove or manipulate non-sensical or missing values.\n",
        "4. Transform values to work with our model.\n",
        "\n",
        "*Question: should we also show how we went about understanding the data?*\n",
        "\n",
        "(Old)The dataset needs a lot of pre processing. Current objective is to clean and remove redundancy and irrelevant columns. In this step we are actively discussing what our goal will be machine learning model, while understanding the data on a deeper level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Excluding irrelevant columns\n",
        "df = df[['Melding ID','Art - gruppe','Meldingsdato','Starttidspunkt','Havdybde start','Havdybde stopp','Rundvekt', 'Hovedområde start', 'Hovedart FAO', 'Trekkavstand']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melding ID           0.000000\n",
            "Art - gruppe         1.631122\n",
            "Meldingsdato         0.000000\n",
            "Starttidspunkt       0.000000\n",
            "Havdybde start       0.000000\n",
            "Havdybde stopp       0.000000\n",
            "Rundvekt             1.629812\n",
            "Hovedområde start    1.350210\n",
            "Hovedart FAO         1.629812\n",
            "Trekkavstand         0.007858\n",
            "dtype: float64\n",
            "Current amount of rows: 296645\n"
          ]
        }
      ],
      "source": [
        "# Calculating the percentage of NaNs for each column to see whether we can afford to drop them\n",
        "nanPercentage = df.isna().mean() * 100\n",
        "print(nanPercentage)\n",
        "df = df.dropna()\n",
        "print(\"Current amount of rows: \" + str(df.shape[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collapsing certain columns?\n",
        "#\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Many of the sea depth notations are positiv, which doesn't make sense.\n",
        "# But the amount of them in relation to number of entries means it can't be discounted as an error\n",
        "# In the lecture on fisheries it was mentioned that a lot fo these are inputed manually\n",
        "# And that most of these non-sensical sea depths are actually correct, just lacking a minus.\n",
        "# Therefore we are simply flipping all the positive sea depth into negatives.\n",
        "df['Havdybde start'] = -df['Havdybde start'].abs()\n",
        "df['Havdybde stopp'] = -df['Havdybde stopp'].abs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current dataset has 12 columns and 65105 rows\n"
          ]
        }
      ],
      "source": [
        "df = df[df['Art - gruppe'].isin(['Torsk', 'Sei', 'Hyse'])]\n",
        "df = df.pivot_table(index=['Melding ID', 'Meldingsdato', 'Starttidspunkt', 'Havdybde start', 'Havdybde stopp','Hovedområde start', 'Hovedart FAO', 'Trekkavstand'], columns='Art - gruppe', values='Rundvekt', aggfunc='sum').reset_index()\n",
        "#?\n",
        "df = df.fillna(0) # filling Nan values with 0\n",
        "df['Hovedfangst'] = df[['Hyse', 'Sei', 'Torsk']].idxmax(axis=1)\n",
        "print(f\"Current dataset has {df.shape[1]} columns and {df.shape[0]} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing decision trees and random forest\n",
        "(Przybys) Having done this and some other test in another file I think we need to go back and look at the data again, because looking closer at \"trekkavstand\" its very bunched up, with a some extreme outliers. Maybe look at adding more feature into use, making a feature importance analysis and normalising them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.4863918412483873\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X = df[['Trekkavstand','Havdybde stopp','Havdybde start']]\n",
        "y = df['Hovedfangst']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=32)\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5623886465564908\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=32)\n",
        "\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred = random_forest.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.52499808 0.46240688 0.48321941 0.48391061 0.46524844] 0.4839566853544275 0.022357701541105612\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "tree = RandomForestClassifier(n_estimators=100, max_depth=30, random_state=32)\n",
        "scores = cross_val_score(tree, X, y, cv=5)\n",
        "print(scores, scores.mean(), scores.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Predicting what the main intended catch is based on the largest rundvekt values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = df[['Hyse', 'Sei', 'Torsk']]\n",
        "y = df['Hovedart FAO']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = knn.score(X_test_scaled, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trying to predict dominant art based on dybde features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "#using knn to predict the fish species based on depth\n",
        "# Step 1: Prepare the data\n",
        "X = df[['Havdybde start', 'Havdybde stopp']]\n",
        "y = df[['Dominant Art']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "accuracy = knn.score(X_test_scaled, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Adding one hot encoded values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#converting Dominant Art column to one hot encoded values\n",
        "dfEncoded = pd.get_dummies(df, columns=['Dominant Art'])\n",
        "dfEncoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#trying to predict the dominant art based on the depth again with one hot encoded values with multioutput, to see if any better\n",
        "X = dfEncoded[['Havdybde start', 'Havdybde stopp']]\n",
        "y = dfEncoded[['Dominant Art_Hyse', 'Dominant Art_Sei', 'Dominant Art_Torsk']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "classifier = MultiOutputClassifier(knn, n_jobs=-1)\n",
        "classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test_scaled)\n",
        "\n",
        "accuracy = classifier.score(X_test_scaled, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Predicting only one dominant art with features dybde start + stopp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = dfEncoded[['Havdybde start', 'Havdybde stopp']]\n",
        "y = dfEncoded[['Dominant Art_Hyse']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "accuracy = knn.score(X_test_scaled, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
